{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa1c2cb-6aa4-477e-8542-417a75682320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl \n",
    "from typing import *\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708b96f0-4f41-4b64-9fd6-a89075f30c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for data storage\n",
    "base_dir = Path(\"/storage/group/izg5139/default/lefteris/\")\n",
    "\n",
    "# Set input and output directories\n",
    "entry_enrichment_input_dir = base_dir / \"multi_species_entry_enrichment_files\"\n",
    "entry_enrichment_output_dir = entry_enrichment_input_dir / \"multi_species_entry_enrichment_results\"\n",
    "formated_reference_mappings_dir = base_dir / 'qp_peptides_over_90_per_phylum/formated_reference_mappings'\n",
    "\n",
    "# Output directories setup\n",
    "output_dirs = {\n",
    "    'multi_species_entry_enrichment_results': entry_enrichment_input_dir / \"multi_species_entry_enrichment_results\",\n",
    "    'phylum_study_domains': entry_enrichment_input_dir / 'phylum_study_domains',\n",
    "    'phylum_study_families': entry_enrichment_input_dir / 'phylum_study_families',\n",
    "    'phylum_background_domains': entry_enrichment_input_dir / 'phylum_background_domains',\n",
    "    'phylum_background_families': entry_enrichment_input_dir / 'phylum_background_families'\n",
    "}\n",
    "\n",
    "# Create all output directories\n",
    "for dir_path in output_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3316d14-420a-4a47-96aa-b76275386d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "taxon_id_mappings = pl.read_parquet(base_dir / \"taxid_mapping.parquet\")\n",
    "interpro_database = pl.read_parquet(entry_enrichment_input_dir / \"interpro_database.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d221b87-4199-4281-ad97-7614ece311bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter interpro database to keep only domain and family entries\n",
    "domains_and_families = interpro_database.filter(\n",
    "    pl.col('Type').is_in(['Domain', 'Family'])).drop('Database', 'Signature', 'Description', 'Representative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707cd65d-a22c-40b7-a887-fb851ff8d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Taxon IDs to the Interpro database\n",
    "interpro_database_with_taxids = domains_and_families.join(\n",
    "            taxon_id_mappings,\n",
    "            on=\"Protein_accession\",\n",
    "            how=\"left\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8cb690-ba11-4f2d-a357-45a4a69da54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mapping_files(directory_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all files matching *_formated_reference_mappings.txt pattern in the given directory and store results in a dictionary.\n",
    "    \"\"\"\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Pattern to match files\n",
    "    pattern = os.path.join(directory_path, \"*_formated_reference_mappings.txt\")\n",
    "    \n",
    "    # Find all matching files\n",
    "    matching_files = glob.glob(pattern)\n",
    "        \n",
    "    # Process each file\n",
    "    for filepath in matching_files:\n",
    "        # Extract the base filename\n",
    "        filename = os.path.basename(filepath)\n",
    "        \n",
    "        # Extract the key (part before \"reference\")\n",
    "        key = filename.split('_formated')[0].strip()\n",
    "        \n",
    "        # Process the file\n",
    "        results[key] = pl.read_csv(filepath, separator ='\\t').rename({'Protein_Name' : 'Protein_name'})\n",
    "    \n",
    "    return results\n",
    "\n",
    "processed_reference_mappings_dict = process_mapping_files(formated_reference_mappings_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93cc019f-c946-44dc-b5fa-71120cc10bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_interpro_by_mappings(\n",
    "    interpro_database_with_taxids: pl.DataFrame,\n",
    "    processed_reference_mappings_dict: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filters InterPro database based on the Taxon IDs present in the Peptide Match reference proteomes mappings dictionary.\n",
    "    \"\"\"\n",
    "    # Pre-compute the set of all unique Taxon_IDs across all mappings\n",
    "    all_taxons = set()\n",
    "    taxon_dict = {}\n",
    "    \n",
    "    # Collect all unique taxons and prepare dictionary\n",
    "    for key, mapping_df in processed_reference_mappings_dict.items():\n",
    "        taxons = set(mapping_df['Taxon_ID'].unique())\n",
    "        taxon_dict[key] = taxons\n",
    "        all_taxons.update(taxons)\n",
    "    \n",
    "    # Single filter operation for all taxons\n",
    "    filtered_base = interpro_database_with_taxids.filter(\n",
    "        pl.col('Taxon_ID').is_in(all_taxons)\n",
    "    )\n",
    "    \n",
    "    # Create filtered results using the filtered base\n",
    "    filtered_results = {}\n",
    "    \n",
    "    for key, taxons in taxon_dict.items():\n",
    "        filtered_results[key] = filtered_base.filter(\n",
    "            pl.col('Taxon_ID').is_in(taxons)\n",
    "        )\n",
    "    return filtered_results\n",
    "\n",
    "filtered_interpro_dict = filter_interpro_by_mappings(\n",
    "    interpro_database_with_taxids,\n",
    "    processed_reference_mappings_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c9ed83-c0fe-48d8-84c9-909a8125cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_background_populations(\n",
    "    filtered_dict: Dict[str, pl.DataFrame]) -> Dict[str, Tuple[pl.DataFrame, pl.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process each Phylum dataframe in the filtered dictionary to extract family and domain populations\n",
    "    and save results to TXT files.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    def extract_phylum_specific_background_populations(phylum_interpro_database: pl.DataFrame) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        This is a helper function meant to be used to extract Protein Domains and Families for each Phylum.\n",
    "        \"\"\"\n",
    "        def extract_population(Type: str) -> pl.DataFrame:\n",
    "            return (\n",
    "                phylum_interpro_database \n",
    "                .filter(pl.col('Type') == Type)\n",
    "                .select(['Protein_name', 'Interpro_description', 'Taxon_ID', \"Interpro_ID\"])\n",
    "                .unique() \n",
    "                .sort('Protein_name')\n",
    "                .rename({\n",
    "                    'Protein_name': \"Protein\", \n",
    "                    'Interpro_description': \"Entry\"\n",
    "                })\n",
    "            )\n",
    "        \n",
    "        families = extract_population('Family')\n",
    "        domains = extract_population('Domain')\n",
    "        \n",
    "        return families, domains\n",
    "    \n",
    "    # Process each phylum and save results\n",
    "    for phylum, df in filtered_dict.items():\n",
    "        # Extract background populations and store to the corresponding dictionary\n",
    "        families, domains = extract_phylum_specific_background_populations(df)\n",
    "        results[phylum] = (families, domains)\n",
    "        \n",
    "        # Save families to file\n",
    "        families_file = Path(output_dirs['phylum_background_families']) / f\"{phylum}_background_families.txt\"\n",
    "        families.write_csv(\n",
    "            families_file,\n",
    "            separator=\"\\t\"\n",
    "        )\n",
    "        \n",
    "        # Save domains to file\n",
    "        domains_file = Path(output_dirs['phylum_background_domains']) / f\"{phylum}_background_domains.txt\"\n",
    "        domains.write_csv(\n",
    "            domains_file,\n",
    "            separator=\"\\t\"\n",
    "        )\n",
    "        \n",
    "    return results\n",
    "\n",
    "background_populations = process_and_save_background_populations(\n",
    "    filtered_interpro_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6337d3-5858-4aca-94da-e5738007a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mapping_and_interpro_data(\n",
    "    mapping_dict: Dict[str, pl.DataFrame],\n",
    "    interpro_dict: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Combines Peptide Match mapping data with InterPro data for each phylum.\n",
    "    \"\"\"\n",
    "    def combine_peptide_match_and_interpro_data(\n",
    "        phylum_mappings: pl.DataFrame, \n",
    "        phylum_interpro_database: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Helper function to combine Dataframes.\n",
    "        \"\"\"\n",
    "        return (phylum_mappings.join(\n",
    "            phylum_interpro_database, \n",
    "            on=\"Protein_name\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "        .sort('Protein_name')\n",
    "        .filter(pl.col('Start').is_not_null())\n",
    "        .filter(pl.col('Interpro_ID').is_not_null()))\n",
    "    \n",
    "    combined_results = {}\n",
    "    \n",
    "    # Ensure we have matching keys in both dictionaries\n",
    "    phyla = set(mapping_dict.keys()) & set(interpro_dict.keys())\n",
    "    \n",
    "    for phylum in phyla:\n",
    "        \n",
    "        # Get the corresponding dataframes\n",
    "        mapping_df = mapping_dict[phylum]\n",
    "        interpro_df = interpro_dict[phylum]\n",
    "        \n",
    "        # Combine the data\n",
    "        combined_df = combine_peptide_match_and_interpro_data(\n",
    "            mapping_df,\n",
    "            interpro_df\n",
    "        )\n",
    "        \n",
    "        combined_results[phylum] = combined_df\n",
    "        \n",
    "    missing_keys = set(mapping_dict.keys()) ^ set(interpro_dict.keys())\n",
    "    if missing_keys:\n",
    "        print(f\"Warning: The following phyla were not found in both dictionaries: {missing_keys}\")\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "mappings_with_entries_dict = combine_mapping_and_interpro_data(\n",
    "    processed_reference_mappings_dict,\n",
    "    filtered_interpro_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a428320-a94e-4b3a-9e97-0c8ed0021400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_family_combinations(\n",
    "    mappings_with_entries_dict: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a new dictionary containing unique combinations of protein families, QP peptides, and protein names for each phylum.\n",
    "    \"\"\"\n",
    "    def extract_unique_families(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Helper function to extract unique protein families\n",
    "        \"\"\"\n",
    "        return (\n",
    "            df\n",
    "            .filter(pl.col(\"Type\") == \"Family\")\n",
    "            .select([\n",
    "                'QP_peptide',\n",
    "                'Protein_name',\n",
    "                'Interpro_description',\n",
    "                'Interpro_ID',\n",
    "                'Taxon_ID'\n",
    "            ])\n",
    "            .unique()\n",
    "            .sort(['Protein_name', 'QP_peptide'])\n",
    "        )\n",
    "    \n",
    "    unique_families_dict = {}\n",
    "    \n",
    "    for phylum, df in mappings_with_entries_dict.items():\n",
    "    \n",
    "        # Extract unique combinations\n",
    "        unique_df = extract_unique_families(df)\n",
    "        unique_families_dict[phylum] = unique_df\n",
    "        \n",
    "    return unique_families_dict\n",
    "\n",
    "phylum_protein_families_dict = extract_unique_family_combinations(\n",
    "    mappings_with_entries_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b98dcc-9a9d-4c7e-af09-0e9c23a02dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domains_from_dictionary(\n",
    "    mappings_with_entries_dict: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extracts QP peptide domains from each Peptide Match dataframe in the dictionary.\n",
    "    \"\"\"\n",
    "    def extract_qp_peptide_domains(peptides_with_interpro_data: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Helper function to check if the Quasi Prime peptide is inside the protein domain or not.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            peptides_with_interpro_data\n",
    "            .filter(\n",
    "                (pl.col(\"Type\") == \"Domain\") &\n",
    "                (pl.col(\"Match_start\") >= pl.col(\"Start\")) &\n",
    "                (pl.col(\"Match_end\") <= pl.col(\"End\"))\n",
    "            )\n",
    "            .drop('Protein_length', 'Protein_accession_right', 'Protein_length_right', 'Start', 'End', 'Taxon_ID_right')\n",
    "        )\n",
    "    \n",
    "    processed_results = {}\n",
    "    \n",
    "    for phylum, df in mappings_with_entries_dict.items():\n",
    "        \n",
    "        processed_df = extract_qp_peptide_domains(df)\n",
    "        processed_results[phylum] = processed_df\n",
    "        \n",
    "    return processed_results\n",
    "\n",
    "phylum_protein_domains_dict = extract_domains_from_dictionary(\n",
    "    mappings_with_entries_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05721f1-90fb-4a1f-8ba7-6391442b7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_study_populations(\n",
    "    phylum_protein_domains_dict: Dict[str, pl.DataFrame],\n",
    "    phylum_protein_families_dict: Dict[str, pl.DataFrame]) -> Dict[str, Tuple[pl.DataFrame, pl.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process each phylum's data to create domain and family populations.\n",
    "    \"\"\"\n",
    "    def format_families(family_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Format the unique families dataframe to match the required structure.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            family_df\n",
    "            .select(['QP_peptide', 'Protein_name', 'Interpro_description', 'Taxon_ID', 'Interpro_ID'])\n",
    "            .rename({\n",
    "                'Protein_name': \"Protein\", \n",
    "                'Interpro_description': \"Entry\"\n",
    "            })\n",
    "        )\n",
    "    \n",
    "    def remove_duplicates_domains(duplicated_dataframe: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Deduplicates domain data.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            duplicated_dataframe\n",
    "            .filter(pl.col('Type') == 'Domain')\n",
    "            .select(['QP_peptide', 'Protein_name', 'Interpro_description', 'Taxon_ID', 'Interpro_ID'])\n",
    "            .unique()\n",
    "            .sort('Protein_name')\n",
    "            .rename({\n",
    "                'Protein_name': \"Protein\", \n",
    "                'Interpro_description': \"Entry\"\n",
    "            })\n",
    "        )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Ensure we have matching keys in both dictionaries\n",
    "    phyla = set(phylum_protein_domains_dict.keys()) & set(phylum_protein_families_dict.keys())\n",
    "    \n",
    "    for phylum in phyla:\n",
    "        \n",
    "        # Get the corresponding dataframes\n",
    "        domains_df = phylum_protein_domains_dict[phylum]\n",
    "        families_df = phylum_protein_families_dict[phylum]\n",
    "        \n",
    "        # Process families and domains\n",
    "        families = format_families(families_df)\n",
    "        domains = remove_duplicates_domains(domains_df)\n",
    "        \n",
    "        results[phylum] = (families, domains)\n",
    "        \n",
    "        # Save families to file\n",
    "        families_file = output_dirs['phylum_study_families'] / f\"{phylum}_study_families.txt\"\n",
    "        families.write_csv(\n",
    "            families_file,\n",
    "            separator=\"\\t\"\n",
    "        )\n",
    "        \n",
    "        # Save domains to file\n",
    "        domains_file = output_dirs['phylum_study_domains'] / f\"{phylum}_study_domains.txt\"\n",
    "        domains.write_csv(\n",
    "            domains_file,\n",
    "            separator=\"\\t\"\n",
    "        )\n",
    "    \n",
    "    # Check if any keys were missing\n",
    "    missing_keys = set(phylum_protein_domains_dict.keys()) ^ set(phylum_protein_families_dict.keys())\n",
    "    if missing_keys:\n",
    "        print(f\"Warning: The following phyla were not found in both dictionaries: {missing_keys}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "study_populations = process_and_save_study_populations(\n",
    "    phylum_protein_domains_dict,\n",
    "    phylum_protein_families_dict\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
