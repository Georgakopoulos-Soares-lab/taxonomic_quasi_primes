{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1028c3-1f57-4bd6-830c-75b4ffb559a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from typing import *\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from scipy.stats import fisher_exact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbfc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for data storage\n",
    "base_dir = Path(\"/storage/group/izg5139/default/lefteris/\")\n",
    "\n",
    "# Set input and output directories\n",
    "entry_enrichment_input_dir = base_dir / \"multi_species_entry_enrichment_files\"\n",
    "entry_enrichment_output_dir = entry_enrichment_input_dir / \"multi_species_entry_enrichment_results\"\n",
    "\n",
    "# Define directories containing the study and background populations for the protein domains and protein families\n",
    "analysis_dirs = {\n",
    "    \"study_domains\": entry_enrichment_input_dir / 'phylum_study_domains',\n",
    "    \"study_families\": entry_enrichment_input_dir / 'phylum_study_families',\n",
    "    \"background_domains\": entry_enrichment_input_dir / 'phylum_background_domains',\n",
    "    \"background_families\": entry_enrichment_input_dir / 'phylum_background_families'\n",
    "}\n",
    "\n",
    "# Define directories meant to store the protein domains and protein families enrichment results \n",
    "output_dirs = {\n",
    "    \"domains\": entry_enrichment_output_dir / 'domain_enrichment_results',\n",
    "    \"families\": entry_enrichment_output_dir / 'family_enrichment_results'\n",
    "}\n",
    "\n",
    "# Create all output directories\n",
    "for dir_path in output_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_phylum_names() -> List[str]:\n",
    "    \"\"\"\n",
    "    Load phylum names from data files into a list.\n",
    "    Handles special phylum names like 'Candidate_division_nc10' and 'Candidatus_absconditabacteria'.\n",
    "    \"\"\"\n",
    "    # Initialize list to store phylum names\n",
    "    phylum_names = []\n",
    "    \n",
    "    # Get all txt files in the directory\n",
    "    files = list(analysis_dirs[\"study_domains\"].glob(\"*.txt\"))\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            # Extract phylum name from filename\n",
    "            file_stem = file_path.stem\n",
    "            \n",
    "            # Split the filename by last underscore to separate phylum name and analysis suffix\n",
    "            parts = file_stem.rsplit('_', 1)\n",
    "            if len(parts) == 2:\n",
    "                phylum_name = parts[0].replace('_study', '')  # Remove '_study' suffix\n",
    "                # Add phylum name to list if not already present\n",
    "                if phylum_name not in phylum_names:\n",
    "                    phylum_names.append(phylum_name)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    return phylum_names\n",
    "\n",
    "phylum_list = load_phylum_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_enrichment_stats(study_count, total_study, background_count, total_background):\n",
    "    \"\"\"\n",
    "    Helper function to calculate enrichment statistics with Haldane-Anscombe correction\n",
    "    \"\"\"\n",
    "    # Calculate the number of entries and non-entries in the study and background sets\n",
    "    study_entry_count = study_count  # Number of entries in the study set\n",
    "    study_non_entry_count = total_study - study_count  # Number of non-entries in the study set\n",
    "    background_entry_count = background_count  # Number of entries in the background set\n",
    "    background_non_entry_count = total_background - background_count  # Number of non-entries in the background set\n",
    "    \n",
    "    # Perform Fisher's exact test to calculate p-value for enrichment\n",
    "    _, p_value = fisher_exact(\n",
    "        [[study_entry_count, study_non_entry_count],\n",
    "         [background_entry_count, background_non_entry_count]],\n",
    "        alternative='greater'  # One-sided test to see if study set is enriched compared to background\n",
    "    )\n",
    "    \n",
    "    # Apply Haldane-Anscombe correction by adding 0.5 to all counts to handle zero counts and reduce bias\n",
    "    ha_study_entry = study_entry_count + 0.5\n",
    "    ha_study_non_entry = study_non_entry_count + 0.5\n",
    "    ha_background_entry = background_entry_count + 0.5\n",
    "    ha_background_non_entry = background_non_entry_count + 0.5\n",
    "    \n",
    "    # Calculate Odds Ratio using the corrected counts\n",
    "    odds_ratio = (ha_study_entry * ha_background_non_entry) / (ha_study_non_entry * ha_background_entry)\n",
    "    # Calculate the log of the Odds Ratio (log-odds ratio), set to NaN if odds ratio is zero or negative\n",
    "    log_or = np.log(odds_ratio) if odds_ratio > 0 else np.nan\n",
    "    \n",
    "    # Calculate the standard error of the log-odds ratio\n",
    "    se_log_or = np.sqrt(\n",
    "        1/ha_study_entry + 1/ha_study_non_entry +\n",
    "        1/ha_background_entry + 1/ha_background_non_entry\n",
    "    )\n",
    "    \n",
    "    # Calculate frequency of entries in study and background sets\n",
    "    study_freq = study_entry_count / total_study if total_study > 0 else 0\n",
    "    background_freq = background_entry_count / total_background if total_background > 0 else 0\n",
    "    # Calculate fold enrichment as the ratio of study frequency to background frequency\n",
    "    fold_enrichment = study_freq / background_freq if background_freq > 0 else np.nan\n",
    "    \n",
    "    # Return all the calculated statistics in a dictionary\n",
    "    return {\n",
    "        'Study_Entry_Counts': study_entry_count,\n",
    "        'Study_Non_Entry_Counts': study_non_entry_count,\n",
    "        'Background_Entry_Counts': background_entry_count,\n",
    "        'Background_Non_Entry_Counts': background_non_entry_count,\n",
    "        'P_Value': p_value,  # p-value from Fisher's exact test\n",
    "        'Odds_Ratio': odds_ratio,  # Odds Ratio for enrichment\n",
    "        'Log_Odds_Ratio': log_or,  # Log-transformed Odds Ratio\n",
    "        'SE_Log_Odds_Ratio': se_log_or,  # Standard Error of Log-Odds Ratio\n",
    "        'Fold_Enrichment': fold_enrichment  # Fold enrichment compared to background\n",
    "    }\n",
    "\n",
    "def process_entries(study_df, background_df, phylum_name, taxon_id):\n",
    "    \"\"\"\n",
    "    Process entries for either domains or families, preserving Interpro_ID information\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "    \n",
    "    # Filter and group the study dataframe, keeping both Entry and Interpro_ID\n",
    "    study_entry_counts = (\n",
    "        study_df.filter(pl.col(\"Taxon_ID\") == taxon_id)\n",
    "        .group_by(['Entry', 'Interpro_ID'])\n",
    "        .len()\n",
    "        .rename({'len': 'Study_Count'})\n",
    "    )\n",
    "    \n",
    "    # Filter and group the background dataframe, keeping both Entry and Interpro_ID\n",
    "    background_entry_counts = (\n",
    "        background_df.filter(pl.col(\"Taxon_ID\") == taxon_id)\n",
    "        .group_by(['Entry', 'Interpro_ID'])\n",
    "        .len()\n",
    "        .rename({'len': 'Background_Count'})\n",
    "    )\n",
    "    \n",
    "    # Perform a left join of background counts with study counts on both 'Entry' and 'Interpro_ID'\n",
    "    merged_counts = background_entry_counts.join(\n",
    "        study_entry_counts, \n",
    "        on=['Entry', 'Interpro_ID'], \n",
    "        how='left'\n",
    "    ).fill_null(0)\n",
    "    \n",
    "    # Calculate totals as before\n",
    "    total_study = study_df.filter(pl.col(\"Taxon_ID\") == taxon_id).shape[0]\n",
    "    total_background = background_df.filter(pl.col(\"Taxon_ID\") == taxon_id).shape[0]\n",
    "    \n",
    "    # Iterate over the merged counts\n",
    "    for row in merged_counts.iter_rows():\n",
    "        # Unpack the entry name, interpro_id, background count, and study count from the row\n",
    "        entry, interpro_id, background_count, study_count = row\n",
    "        \n",
    "        try:\n",
    "            # Calculate enrichment statistics\n",
    "            stats = calculate_enrichment_stats(\n",
    "                int(study_count),\n",
    "                total_study,\n",
    "                int(background_count),\n",
    "                total_background\n",
    "            )\n",
    "            \n",
    "            # Append results including the Interpro_ID\n",
    "            results.append({\n",
    "                'Phylum': phylum_name,\n",
    "                'Taxon_ID': taxon_id,\n",
    "                'Entry': entry,\n",
    "                'Interpro_ID': interpro_id,  # Include Interpro_ID in the output\n",
    "                **stats\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry {entry} (Interpro_ID: {interpro_id}) for taxon {taxon_id}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path):\n",
    "    \"\"\"\n",
    "    Save results to TXT file with multiple testing correction, even if empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not results:\n",
    "            # Create an empty DataFrame with all expected columns\n",
    "            empty_df = pl.DataFrame(schema={\n",
    "                'Phylum': pl.Utf8,\n",
    "                'Taxon_ID': pl.Int64,\n",
    "                'Entry': pl.Utf8,\n",
    "                'Study_Entry_Counts': pl.Int64,\n",
    "                'Study_Non_Entry_Counts': pl.Int64,\n",
    "                'Background_Entry_Counts': pl.Int64,\n",
    "                'Background_Non_Entry_Counts': pl.Int64,\n",
    "                'Odds_Ratio': pl.Float64,\n",
    "                'Log_Odds_Ratio': pl.Float64,\n",
    "                'SE_Log_Odds_Ratio': pl.Float64,\n",
    "                'P_Value': pl.Float64,\n",
    "                'Fold_Enrichment': pl.Float64\n",
    "            })\n",
    "            # Save the empty DataFrame\n",
    "            empty_df.write_csv(output_path, separator='\\t')\n",
    "            return\n",
    "\n",
    "        # Create a DataFrame from the results list and cast columns to appropriate types\n",
    "        df = pl.DataFrame(results).with_columns([\n",
    "            pl.col('Taxon_ID').cast(pl.Int64),\n",
    "            pl.col('Entry').cast(pl.Utf8),\n",
    "            pl.col('Study_Entry_Counts').cast(pl.Int64),\n",
    "            pl.col('Study_Non_Entry_Counts').cast(pl.Int64),\n",
    "            pl.col('Background_Entry_Counts').cast(pl.Int64),\n",
    "            pl.col('Background_Non_Entry_Counts').cast(pl.Int64),\n",
    "            pl.col('Odds_Ratio').cast(pl.Float64),\n",
    "            pl.col('Log_Odds_Ratio').cast(pl.Float64),\n",
    "            pl.col('SE_Log_Odds_Ratio').cast(pl.Float64),\n",
    "            pl.col('P_Value').cast(pl.Float64),\n",
    "            pl.col('Fold_Enrichment').cast(pl.Float64)\n",
    "        ])\n",
    "        \n",
    "        # Filter for p-value < 0.1\n",
    "        filtered_df = df.filter(pl.col('P_Value') < 0.1)\n",
    "        \n",
    "        # If filtered DataFrame is empty, save the empty DataFrame with column headers\n",
    "        if filtered_df.is_empty():\n",
    "            # Create empty DataFrame with same schema\n",
    "            empty_df = pl.DataFrame(schema=df.schema)\n",
    "            empty_df.write_csv(output_path, separator='\\t')\n",
    "        else:\n",
    "            # Save the filtered DataFrame\n",
    "            filtered_df.write_csv(output_path, separator='\\t')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to {output_path}: {str(e)}\")\n",
    "        # Create and save empty DataFrame even in case of error\n",
    "        empty_df = pl.DataFrame(schema={\n",
    "            'Phylum': pl.Utf8,\n",
    "            'Taxon_ID': pl.Int64,\n",
    "            'Entry': pl.Utf8,\n",
    "            'Study_Entry_Counts': pl.Int64,\n",
    "            'Study_Non_Entry_Counts': pl.Int64,\n",
    "            'Background_Entry_Counts': pl.Int64,\n",
    "            'Background_Non_Entry_Counts': pl.Int64,\n",
    "            'Odds_Ratio': pl.Float64,\n",
    "            'Log_Odds_Ratio': pl.Float64,\n",
    "            'SE_Log_Odds_Ratio': pl.Float64,\n",
    "            'P_Value': pl.Float64,\n",
    "            'Fold_Enrichment': pl.Float64\n",
    "        })\n",
    "        empty_df.write_csv(output_path, separator='\\t')\n",
    "\n",
    "        \n",
    "def perform_phylum_enrichment_analysis(phylum_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs enrichment analysis for both domains and families for a specific phylum.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read input data\n",
    "    study_domains = pl.read_csv(analysis_dirs[\"study_domains\"] / f\"{phylum_name}_study_domains.txt\", separator='\\t').with_columns([pl.col(\"Taxon_ID\").cast(pl.Int64)])\n",
    "    background_domains = pl.read_csv(analysis_dirs[\"background_domains\"] / f\"{phylum_name}_background_domains.txt\", separator='\\t').with_columns([pl.col(\"Taxon_ID\").cast(pl.Int64)])\n",
    "    study_families = pl.read_csv(analysis_dirs[\"study_families\"] / f\"{phylum_name}_study_families.txt\", separator='\\t').with_columns([pl.col(\"Taxon_ID\").cast(pl.Int64)])\n",
    "    background_families = pl.read_csv(analysis_dirs[\"background_families\"] / f\"{phylum_name}_background_families.txt\", separator='\\t').with_columns([pl.col(\"Taxon_ID\").cast(pl.Int64)])\n",
    "    \n",
    "    # Get unique taxon IDs\n",
    "    taxon_ids = set(np.concatenate([\n",
    "        study_domains['Taxon_ID'].unique(),\n",
    "        study_families['Taxon_ID'].unique()\n",
    "    ]))\n",
    "    \n",
    "    \n",
    "    domain_results = []\n",
    "    family_results = []\n",
    "    \n",
    "    # Process each taxon ID\n",
    "    for taxon_id in taxon_ids:\n",
    "        try:\n",
    "            # Process domains\n",
    "            domain_results.extend(\n",
    "                process_entries(study_domains, background_domains, phylum_name, taxon_id)\n",
    "            )\n",
    "            \n",
    "            # Process families\n",
    "            family_results.extend(\n",
    "                process_entries(study_families, background_families, phylum_name, taxon_id)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing taxon {taxon_id} in phylum {phylum_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    save_results(domain_results, output_dirs['domains'] / f\"{phylum_name}_domains_enrichment_results.txt\")\n",
    "    save_results(family_results, output_dirs['families'] / f\"{phylum_name}_families_enrichment_results.txt\")\n",
    "    \n",
    "    return f\"Completed {phylum_name}\"\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Get list of all phyla and their data\n",
    "        total_phyla = len(phylum_list)\n",
    "\n",
    "        print(f\"\\nStarting processing {total_phyla} phyla...\")\n",
    "\n",
    "       # Create process pool and process phyla\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            # Use imap_unordered to process phyla in parallel\n",
    "            for i, _ in enumerate(pool.imap_unordered(perform_phylum_enrichment_analysis, phylum_list), 1):\n",
    "                print(f\"Completed {i}/{total_phyla} phyla\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parallel processing: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
